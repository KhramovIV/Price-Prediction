{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce5009f",
   "metadata": {},
   "source": [
    "Решим задачу с помощью GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42b00546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: tensor([[[ 0.0000e+00, -4.5221e-04,  1.7646e-02],\n",
      "         [-8.9999e-03,  1.4142e-04,  4.5124e-04],\n",
      "         [-8.7464e-03,  2.1406e-01,  3.5301e-03],\n",
      "         ...,\n",
      "         [-1.6479e-02,  4.5778e-01,  1.8536e-04],\n",
      "         [-1.6479e-02,  1.9380e-04,  1.4391e-03],\n",
      "         [-1.4070e-02, -3.6771e-01,  5.6405e-03]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.4142e-04,  4.5124e-04],\n",
      "         [ 2.5582e-04,  2.1406e-01,  3.5301e-03],\n",
      "         [ 2.5582e-04,  1.4937e-01,  1.9924e-04],\n",
      "         ...,\n",
      "         [-7.5467e-03,  1.9380e-04,  1.4391e-03],\n",
      "         [-5.1164e-03, -3.6771e-01,  5.6405e-03],\n",
      "         [-2.9419e-03,  1.8459e-01,  1.0740e-03]],\n",
      "\n",
      "        [[ 0.0000e+00,  2.1406e-01,  3.5301e-03],\n",
      "         [ 0.0000e+00,  1.4937e-01,  1.9924e-04],\n",
      "         [ 6.3944e-04,  9.8597e-02,  4.7762e-04],\n",
      "         ...,\n",
      "         [-5.3709e-03, -3.6771e-01,  5.6405e-03],\n",
      "         [-3.1970e-03,  1.8459e-01,  1.0740e-03],\n",
      "         [-2.5576e-04, -5.2848e-03,  4.0681e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000e+00, -2.9088e-03,  6.3242e-02],\n",
      "         [ 1.8399e-03,  5.2675e-02,  7.1817e-03],\n",
      "         [ 6.3083e-03,  9.9524e-01,  4.1514e-03],\n",
      "         ...,\n",
      "         [ 2.8913e-03, -2.3880e-03,  2.7152e-01],\n",
      "         [ 2.8913e-03, -2.8545e-01,  1.5828e-03],\n",
      "         [ 4.5998e-03, -3.3258e-03,  2.3499e-03]],\n",
      "\n",
      "        [[ 0.0000e+00,  5.2675e-02,  7.1817e-03],\n",
      "         [ 4.4602e-03,  9.9524e-01,  4.1514e-03],\n",
      "         [ 3.5419e-03,  1.4247e-04,  4.7079e-02],\n",
      "         ...,\n",
      "         [ 1.0494e-03, -2.8545e-01,  1.5828e-03],\n",
      "         [ 2.7549e-03, -3.3258e-03,  2.3499e-03],\n",
      "         [ 2.7549e-03,  4.8513e-03,  4.1445e-04]],\n",
      "\n",
      "        [[ 0.0000e+00,  9.9524e-01,  4.1514e-03],\n",
      "         [-9.1422e-04,  1.4247e-04,  4.7079e-02],\n",
      "         [ 6.6606e-03,  1.4206e-01,  1.7427e-02],\n",
      "         ...,\n",
      "         [-1.6978e-03, -3.3258e-03,  2.3499e-03],\n",
      "         [-1.6978e-03,  4.8513e-03,  4.1445e-04],\n",
      "         [-2.3507e-03, -6.8271e-02,  1.9410e-02]]])\n",
      "Target shape: tensor([-1.0000, -1.6500, -3.3000,  0.9000,  3.4000,  3.4000,  5.4000,  6.3500,\n",
      "         0.1000,  0.5500,  1.0500,  2.9000,  2.9000,  2.4500,  1.6500,  1.6500,\n",
      "         1.8000,  1.8000,  1.8500,  1.8500,  1.8500,  1.8500,  2.0500,  2.0500,\n",
      "         1.1000,  1.1000,  0.7000, -0.1000, -0.1000, -0.8000, -0.8000, -0.7500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/data_proc.csv\")\n",
    "df = df.dropna(subset=['close', 'close + 1 hour', 'sent_scores'])\n",
    "df['delta'] = df['close + 1 hour'] - df['close']\n",
    "\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, window_size=60):\n",
    "        self.window_size = window_size\n",
    "        self.df = df\n",
    "        \n",
    "        self.scaler_views = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        self.raw_prices = self.df['close'].values.astype(np.float32)\n",
    "        \n",
    "        raw_views = self.df['views'].values.reshape(-1, 1)\n",
    "        self.views_norm = self.scaler_views.fit_transform(raw_views).flatten()\n",
    "        \n",
    "        self.sent_norm = self.df['sent_scores'].values.astype(np.float32)\n",
    "        \n",
    "        self.targets = self.df['delta'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        raw_price_sequence = self.raw_prices[idx : idx + self.window_size]\n",
    "        \n",
    "        price_start = raw_price_sequence[0]\n",
    "        if price_start == 0:\n",
    "            price_norm_sequence = raw_price_sequence\n",
    "        else:\n",
    "            price_norm_sequence = (raw_price_sequence / price_start) - 1.0\n",
    "            \n",
    "        sent_sequence = self.sent_norm[idx : idx + self.window_size]\n",
    "        views_sequence = self.views_norm[idx : idx + self.window_size]\n",
    "        \n",
    "        x_sequence = np.column_stack([\n",
    "            price_norm_sequence, \n",
    "            sent_sequence, \n",
    "            views_sequence\n",
    "        ])\n",
    "        \n",
    "        y_target = self.targets[idx + self.window_size - 1]\n",
    "        \n",
    "        return (torch.tensor(x_sequence, dtype=torch.float32), \n",
    "                torch.tensor(y_target, dtype=torch.float32))\n",
    "\n",
    "dataset = TimeSeriesDataset(df, window_size=60)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for seq, target in dataloader:\n",
    "    print(f\"Input shape: {seq}\")\n",
    "    print(f\"Target shape: {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['views', 'close', 'close + -10 minutes', 'close + -20 minutes', 'close + -30 minutes', 'close + -40 minutes', 'close + -50 minutes', 'close + -60 minutes', 'close + -70 minutes', 'close + -80 minutes', 'close + -90 minutes', 'close + -100 minutes', 'close + -110 minutes', 'close + -120 minutes', 'close + -130 minutes', 'close + -140 minutes', 'close + -150 minutes', 'close + -160 minutes', 'close + -170 minutes', 'close + -180 minutes', 'close + -190 minutes', 'sent_scores', 'delta']\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"./data/data_proc.csv\")\n",
    "# df['delta'] = df['close + 1 hour'] - df['close']\n",
    "# df = df.dropna()\n",
    "# features = [feature for feature in df.columns if 'close' or 'sent_scores' or 'views' in feature]\n",
    "# features.remove('close + 1 hour')\n",
    "# features.remove('close + 30 minutes')\n",
    "# features.remove('date')\n",
    "# features.remove('Unnamed: 0')\n",
    "# features.remove('sent_labels')\n",
    "# seq_columns = [feature for feature in features if 'close' in feature]\n",
    "# print(features)\n",
    "\n",
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.scaler_price = MinMaxScaler()\n",
    "#         self.scaler_views = MinMaxScaler() \n",
    "        \n",
    "#         all_prices = np.concatenate([self.df[col].values for col in seq_columns]).reshape(-1, 1)\n",
    "#         self.scaler_price.fit(all_prices)\n",
    "        \n",
    "#         self.df['views_norm'] = self.scaler_views.fit_transform(self.df['views'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "#         sent = row['sent_scores']\n",
    "#         views = row['views_norm']\n",
    "#         prices = row['close']\n",
    "        \n",
    "#         #prices = [row[feature] for feature in seq_columns]\n",
    "#         prices_norm = self.scaler_price.transform(np.array(prices).reshape(-1, 1)).flatten()\n",
    "        \n",
    "#         sequence_length = len(prices_norm)\n",
    "#         seq_data = np.column_stack([\n",
    "#             prices,\n",
    "#             sent,\n",
    "#             views        \n",
    "#         ])\n",
    "#         seq = torch.tensor(seq_data, dtype=torch.float32)\n",
    "        \n",
    "#         target = torch.tensor(row['delta'], dtype=torch.float32) \n",
    "#         return seq, target\n",
    "\n",
    "# dataset = TimeSeriesDataset(df)\n",
    "# dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f0f3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=8, num_layers=6, output_size=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, input_size]\n",
    "        out, hn = self.gru(x)\n",
    "        final_out = self.fc(out[:, -1, :]) \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.6696, Val Loss: 11.4927\n"
     ]
    }
   ],
   "source": [
    "df_t = df #.iloc[:int(0.5 * len(df))]\n",
    "train_size = int(0.8 * len(df_t))\n",
    "df_train = df_t.iloc[:train_size]\n",
    "df_val = df_t.iloc[train_size:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(df_train)\n",
    "val_dataset = TimeSeriesDataset(df_val) \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32) \n",
    "\n",
    "model = GRUModel(input_size=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_seq, batch_target in train_dataloader:\n",
    "        batch_seq, batch_target = batch_seq.to(device), batch_target.to(device)\n",
    "        #print(batch_seq.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_seq)\n",
    "        loss = criterion(output.squeeze(), batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_seq, batch_target in val_dataloader:\n",
    "            batch_seq, batch_target = batch_seq.to(device), batch_target.to(device)\n",
    "            output = model(batch_seq)\n",
    "            loss = criterion(output.squeeze(), batch_target)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "059c3174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3396.2359, Val Loss: 15402.4831\n",
      "Epoch 2, Train Loss: 3130.6564, Val Loss: 14454.2634\n",
      "Epoch 3, Train Loss: 2915.3867, Val Loss: 13606.8082\n",
      "Epoch 4, Train Loss: 2743.5901, Val Loss: 12853.8767\n",
      "Epoch 5, Train Loss: 2608.7521, Val Loss: 12188.9420\n",
      "Epoch 6, Train Loss: 2504.8216, Val Loss: 11605.5491\n",
      "Epoch 7, Train Loss: 2426.2589, Val Loss: 11096.9431\n",
      "Epoch 8, Train Loss: 2368.0982, Val Loss: 10656.5401\n",
      "Epoch 9, Train Loss: 2326.0285, Val Loss: 10277.7114\n",
      "Epoch 10, Train Loss: 2296.3583, Val Loss: 9953.9987\n",
      "Epoch 11, Train Loss: 2276.0453, Val Loss: 9679.2767\n",
      "Epoch 12, Train Loss: 2262.6062, Val Loss: 9447.6141\n",
      "Epoch 13, Train Loss: 2254.1091, Val Loss: 9253.4608\n",
      "Epoch 14, Train Loss: 2249.0358, Val Loss: 9091.6300\n",
      "Epoch 15, Train Loss: 2246.3013, Val Loss: 8957.5197\n",
      "Epoch 16, Train Loss: 2245.0934, Val Loss: 8847.0083\n",
      "Epoch 17, Train Loss: 2244.8260, Val Loss: 8756.3067\n",
      "Epoch 18, Train Loss: 2245.1183, Val Loss: 8682.1676\n",
      "Epoch 19, Train Loss: 2245.7130, Val Loss: 8621.8255\n",
      "Epoch 20, Train Loss: 2246.4307, Val Loss: 8572.8578\n",
      "Epoch 21, Train Loss: 2247.1792, Val Loss: 8533.2554\n",
      "Epoch 22, Train Loss: 2247.8985, Val Loss: 8501.2993\n",
      "Epoch 23, Train Loss: 2248.5503, Val Loss: 8475.5684\n",
      "Epoch 24, Train Loss: 2249.1275, Val Loss: 8454.8886\n",
      "Epoch 25, Train Loss: 2249.6294, Val Loss: 8438.3097\n",
      "Epoch 26, Train Loss: 2250.0573, Val Loss: 8425.0275\n",
      "Epoch 27, Train Loss: 2250.4170, Val Loss: 8414.4189\n",
      "Epoch 28, Train Loss: 2250.7142, Val Loss: 8405.9270\n",
      "Epoch 29, Train Loss: 2250.9605, Val Loss: 8399.1292\n",
      "Epoch 30, Train Loss: 2251.1714, Val Loss: 8393.6952\n",
      "Epoch 31, Train Loss: 2251.3379, Val Loss: 8389.3538\n",
      "Epoch 32, Train Loss: 2251.4785, Val Loss: 8385.9187\n",
      "Epoch 33, Train Loss: 2251.5930, Val Loss: 8383.1510\n",
      "Epoch 34, Train Loss: 2251.6832, Val Loss: 8380.9287\n",
      "Epoch 35, Train Loss: 2251.7623, Val Loss: 8379.1934\n",
      "Epoch 36, Train Loss: 2251.8244, Val Loss: 8377.7706\n",
      "Epoch 37, Train Loss: 2251.8783, Val Loss: 8376.6861\n",
      "Epoch 38, Train Loss: 2251.9187, Val Loss: 8375.7976\n",
      "Epoch 39, Train Loss: 2251.9521, Val Loss: 8375.0678\n",
      "Epoch 40, Train Loss: 2251.9758, Val Loss: 8374.4649\n",
      "Epoch 41, Train Loss: 2252.0040, Val Loss: 8373.9889\n",
      "Epoch 42, Train Loss: 2252.0213, Val Loss: 8373.6187\n",
      "Epoch 43, Train Loss: 2252.0365, Val Loss: 8373.3279\n",
      "Epoch 44, Train Loss: 2252.0470, Val Loss: 8373.0636\n",
      "Epoch 45, Train Loss: 2252.0622, Val Loss: 8372.9050\n",
      "Epoch 46, Train Loss: 2252.0689, Val Loss: 8372.7569\n",
      "Epoch 47, Train Loss: 2252.0735, Val Loss: 8372.6141\n",
      "Epoch 48, Train Loss: 2252.0838, Val Loss: 8372.5401\n",
      "Epoch 49, Train Loss: 2252.0871, Val Loss: 8372.4713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_seq, batch_target \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[32m     20\u001b[39m     batch_seq, batch_target = batch_seq.to(device), batch_target.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     loss = criterion(output.squeeze(), batch_target)\n\u001b[32m     23\u001b[39m     val_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ваня\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ваня\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mGRUModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# x: [batch, seq_len, input_size]\u001b[39;00m\n\u001b[32m     11\u001b[39m     out, hn = \u001b[38;5;28mself\u001b[39m.gru(x)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     final_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ваня\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ваня\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ваня\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Train phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_seq, batch_target in train_dataloader:\n",
    "        batch_seq, batch_target = batch_seq.to(device), batch_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_seq)\n",
    "        loss = criterion(output.squeeze(), batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_seq, batch_target in val_dataloader:\n",
    "            batch_seq, batch_target = batch_seq.to(device), batch_target.to(device)\n",
    "            output = model(batch_seq)\n",
    "            loss = criterion(output.squeeze(), batch_target)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebc44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
